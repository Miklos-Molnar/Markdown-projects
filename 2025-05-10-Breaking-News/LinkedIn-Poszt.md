Breaking News: **Amikor a g√©p elkezd n√©lk√ºl√ºnk gondolkodni**, 2025.m√°jus 7.
 
Egy k√≠nai nemzetk√∂zi kutat√≥csoport most mutatta be azt, amit sokan √©vekig lehetetlennek hittek: az adatok n√©lk√ºli gondolkod√°st. Nem memoriz√°l√°s, nem ut√°nz√°s, hanem **m√©ly, iterat√≠v, szintetikus** √∂nj√°t√©k. A k√∂vetkezm√©nyek? √Åltal√°nosan revelat√≠v √©s relevat√≠v (aka relev√°ns) lehet, matematik√°ra, tudom√°nyra. Tal√°n m√©g arra is, ahogyan mag√°t a tanul√°st defini√°ljuk.

üëá √çme, mi t√∂rt√©nt, √©s mi√©rt lehet ez a **legfontosabb mesters√©ges intelligencia m√©rf√∂ldk≈ë az AlphaGo √≥ta**.

Minimum 1 √©ve tudhat√≥, diskurzus t√°rgy√°t k√©pez≈ë volt, hogy el fog j√∂nni a nap, √©s el is j√∂tt. J√≥val hamarabb, mint gondoltam volna. √ârz√©kelhet≈ëen, neg√°ll√≠thatatlan a fejl≈ëd√©s. M√©gha csak b√©ta is, m√©gha csak n√ºansznyi is lehet a performancia-javul√°s, m√©gha csak gyerekcip≈ëben lehet is az eg√©sz, de legal√°bb '**first draft implemented**'. R√°ad√°sul m√©gcsak nem is human in-the-loop, hanem egyenesen **human on-the-loop** ir√°nyba mozdulva. Az √©sz meg√°ll, komolyan. R√∂gz√≠ts√ºk: 2025.m√°jus7-√©n. Szegedy Kriszti√°n (xAi) 2026-os c√©ld√°tuma el≈ëtt b≈ë m√°sf√©l √©vvel.

11 k√≠nai cikke: Tsinghua University, Beijing Institute for General Artificial Intelligence & Pennsylvania State University egyetemekr≈ël
**ABSOLUTE ZERO: REINFORCED SELF-PLAY REASONING WITH ZERO DATA**
https://arxiv.org/html/2505.03335v2

Itt egy 15 percnyi magyar√°z√≥ YT-vide√≥ a cikkhez:
**New ‚ÄûAbsolute Zero‚Äù Model Learns with NO DATA**
https://www.youtube.com/watch?v=CqdqZNqljdI

TL;DR - Na √©s mir≈ël lenne sz√≥ min√©l kevesebb √°m min√©l olvasm√°nyosabb bet≈±vel, siet≈ëseknek?

(1) Tegy√ºk fel van egy matematikus, aki **nyitott matematikai probl√©m√°kat akar megoldani**. Sz√©ls≈ë hat√°ron mondjuk egy Riemann-sejt√©st, az olyan j√≥ dem√≥-p√©lda. 

(2) Mindehhez van neki, vagy tervez fejben √©s/vagy brainstormingban egy megold√°si koncepci√≥t, mondjuk **particion√°l√°ssal (dekompon√°l√°ssal)**. Lebontva nagyobb, majd kisebb l√©p√©sekre, t√©telekre, lemm√°kra, propozici√≥kra, egzaktan t√°rgyalhat√≥ √∂sszef√ºgg√©sekre, stb., mondjuk most √≠gy "atomi" szintig. Amik egy√©bk√©nt felt√©telezhet≈ëen - az AI sz√°m√°ra is - m√©g √≠gy is kezelhetetlen√ºl komplexek. Egy ilyen matematikus nemcsak hallott m√°r az AI-r√≥l, de keresi is az AI "bar√°ts√°g√°t" az ilyen matek-mel√≥hoz. P√©ld√°ul Terence Tao, 230-as IQ-val. N√©h√°nyat saj√°t k√©zben tart, m√°sokat deleg√°l koll√©g√°inak, beosztottainak, priv√°t m√≥don, vagy publikus f√≥rumon kereszt√ºl.

(3) A csapat tagjai bek√ºldik az "atomi" probl√©m√°t valamilyen **LLM-es Deep Reasearch** cs≈ëvezet√©kbe. √Åm eddig j√≥ es√©llyel 90+%-ban az LLM fejrefog √°llni, AI-s megoldhatatlans√°g ok√°n. Eddig ugye ezt eddig is tudta az AI. üôÇ

(4) J√∂n a **friss k√≠nai √≠g√©rv√©ny**: a bet√°pl√°lt probl√©m√°hoz, egy AI-agent(ek), ami(k) kigener√°l(nak) **szintetikus training adatok**at pluszba a kor√°bbi megl√©v≈ë adatok mell√©, majd ezek ut√°n az √≠gy kib≈ëv√≠tett - inkrement√°lis addicion√°lis trainingbe bevont - adatokon pr√≥b√°lja folytatni a Deep Research-√∂t (pontoss√°gi, tanulhat√≥s√°gi, stb. **jutalmaz√°si rendszer**ben, a **RL=reinforcement learning keretei k√∂z√∂tt**).

(5) Ugye nem neh√©z elk√©pzelni, hogy   
  * az **iter√°ci√≥ kv√°zi v√©gtelenszer ism√©telhet≈ë**, a nap 24 √≥r√°j√°ban.  
  * A sz√°m√≠t√°si kapacit√°s b√°tran **felsk√°l√°zhat√≥** (a **m√≥dszer tuningol√°s√°**val p√°rhuzamosan),   
  * mik√∂zben a DeepSeek 671 milli√°rd param√©teres legnagyobb modellje is futhat **home pc**-n, 220V-on, r√©szben √©jszakai tarif√°val.  
  * √âs ez m√©g csak az **els≈ë implement√°l√°s**i l√©p√©s volt (megm√©rt √©s kimutatott fejl≈ëd√©ssel).  
  * √âs akkor m√©g nem besz√©lt√ºnk az **NVidia** tervezett **Rubin architekt√∫r√°j√∫ GPU**-jair√≥l.  
(+1) Itt **jegyzem meg**, hogy √©les szakmai vit√°knak lehet√ºnk szemtanui: Google Deepmind-n√°l (AlphaZero, MuZero b√∂lcs≈ën√©l), az RF-nek nem nagyon vannak bar√°tai. M√≠g a k√≠naiak l√°tnival√≥an nagyon nyomj√°k az RL-t, akkora potenci√°lt √©rz√©kelnek benne. Nyilv√°n nem ennyire fekete-feh√©r a dolog, de a verseny ezen a t√©ren is nagyon jelent≈ës (az √©n szememben mindenk√©ppen).

A cikk (szik√°r, t√∂m√∂r, elevator pitch) konkl√∫zi√≥ja sz√°momra:
(a) **K√°r scaling wallr√≥l besz√©lni m√©g sz√∂veges adatokn√°l is**, amit p√°r h√≥napra el≈ëre v√°r Sam Altman (OpenAI).
(b) Szintetikus adatok, √ñnjutalmaz√≥ tanul√°s, Verfik√°lhat√≥ erem√©nyek **h√°rmasa k√©zzelfoghat√≥ szimb√≥zisba hozhat√≥k**.
(c) Mindh√°romf√©le √©rvel√©si  (**dedukt√≠v, indukt√≠v √©s abdukt√≠v**) k√©pess√©g integr√°lhat√≥v√° tehet≈ë.
(d) **Invari√°ns a modell-m√©ret**, amire lehetne mindezt alkalmazni. Lelkiszemeimmel l√°tom a matematikust, a mobiltelefonj√°n l√©v≈ë mondjuk Microsoft Phi-4-gyel, √©s konferenci√°ra utaz√°s k√∂zben dolgozik a Riemann-sejt√©sen.
(e) Kiemelked≈ë lehet teh√°t a teljes√≠tm√©ny k√ºls≈ë adatok n√©lk√ºl is. **Na most elk√©pzelhet≈ë milyen lesz, ha be is vonunk tov√°bbi k√ºls≈ë adatokat is**.
